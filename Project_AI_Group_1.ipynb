{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Artificial Intelligence Project\n",
    "\n",
    "## Group 1:\n",
    "### - Rajoelisoa Enorian\n",
    "### - Hirli Baptiste\n",
    "### - Bhattacharjee Ankit\n",
    "### - Caumartin Evan"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "### 1. Load the data\n",
    "### 2. Merge the data\n",
    "### 3. Calculate the average working time\n",
    "### 4. Remove the outliers\n",
    "### 5. Drop the unnecessary columns\n",
    "### 6. Encode the categorical data\n",
    "### 7. Impute the missing values\n",
    "### 8. Scale the data\n",
    "### 9. Save the data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ],
   "metadata": {
    "id": "lVFYaugeLQiE"
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_employee_data: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "is_ethic = False"
   ],
   "metadata": {
    "id": "Dg2ybgF8LpMd"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the data\n",
    "\n",
    "### Load the data from the datasets folder\n",
    "\n",
    "We read multiple CSV files into pandas DataFrames. The general_data, employee_survey_data, and manager_survey_data DataFrames contain general, employee survey, and manager survey data, respectively. The in_time and out_time DataFrames contain timestamp data for employee check-in and check-out times, which are converted to datetime format."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "general_data = pd.read_csv('datasets/general_data.csv')\n",
    "employee_survey_data = pd.read_csv('datasets/employee_survey_data.csv')\n",
    "manager_survey_data = pd.read_csv('datasets/manager_survey_data.csv')\n",
    "in_time: pd.DataFrame = pd.read_csv('datasets/in_time.csv').astype('datetime64[s]')\n",
    "out_time = pd.read_csv('datasets/out_time.csv').astype('datetime64[s]')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Merge the data\n",
    "\n",
    "We are merging general data, employee survey data, and manager survey data into a single DataFrame."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_employee_data = general_data.merge(employee_survey_data, on='EmployeeID')\n",
    "full_employee_data = full_employee_data.merge(manager_survey_data, on='EmployeeID')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Calculate the average working time\n",
    "\n",
    "In this deliverable, we analyzed employee working patterns by calculating key metrics such as average arrival time, departure time, and working hours. Starting with arrival (`in_time`) and departure (`out_time`) datasets, we determined the total time spent at work each day by subtracting arrival times from departure times and converting the results into hours.\n",
    "We then calculated the **AverageArrivalTime** (mean of arrival times), **AverageDepartureTime** (latest departure time), and **AverageWorkingTime** (average daily working hours) for each employee. These insights were combined into a new dataset and merged with the general employee data to provide a comprehensive view, including details like name and department alongside their working time metrics.\n",
    "For instance, an employee arriving at 9:15 AM and leaving at 6:30 PM consistently would have an average working time of 9 hours. This analysis helps in understanding employee work habits, identifying trends, and improving operational efficiency."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "average: pd.DataFrame = (out_time - in_time)\n",
    "\n",
    "# Convert to hours\n",
    "average = average.loc[:, :] / np.timedelta64(1, 'h')\n",
    "\n",
    "working_time_df = pd.DataFrame()\n",
    "\n",
    "# Create a column EmployeeID\n",
    "working_time_df['EmployeeID'] = in_time.iloc[:, 0]\n",
    "working_time_df['EmployeeID'] = working_time_df['EmployeeID'].astype('int64')\n",
    "\n",
    "# Create a column min and max\n",
    "working_time_df['AverageArrivalTime'] = in_time.iloc[:, 1:].mean(axis=1)\n",
    "working_time_df['AverageDepartureTime'] = out_time.iloc[:, 1:].max(axis=1)\n",
    "\n",
    "# Create a column average\n",
    "working_time_df['AverageWorkingTime'] = average.mean(axis=1).round(2)\n",
    "\n",
    "# Merge the working time data with the general data\n",
    "full_employee_data = full_employee_data.merge(working_time_df, on='EmployeeID')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Remove the outliers\n",
    "\n",
    "We removed the outliers from the dataset using the Interquartile Range (IQR) method. Outliers are data points that significantly differ from other observations in the dataset. They can skew the results of statistical analyses and machine learning models.\n",
    "The IQR method identifies outliers by calculating the range between the first and third quartiles of the data and flagging values that fall below the lower bound (Q1 - 1.5 * IQR) or above the upper bound (Q3 + 1.5 * IQR). Outliers are then replaced with the lower or upper bound values to ensure they do not impact the analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def find_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (series < lower_bound) | (series > upper_bound)\n",
    "    return outliers\n",
    "\n",
    "def cap_outliers_in_dataframe(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    for column in df.columns:\n",
    "        if np.issubdtype(df[column].dtype, np.number):  # Check if the column contains numeric data\n",
    "            outliers = find_outliers_iqr(df[column])\n",
    "\n",
    "            # Cap the outliers by replacing them with lower/upper bounds\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            df[column] = np.where(outliers, np.clip(df[column], lower_bound, upper_bound), df[column])\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_employee_data = cap_outliers_in_dataframe(full_employee_data)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display the first 5 rows of the data\n",
    "\n",
    "We displayed the first five rows of the dataset to understand its structure and contents. This step provides an overview of the data, including the column names, data types, and values. It helps in identifying any potential issues, such as missing values, outliers, or incorrect data types."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "full_employee_data.head()"
   ],
   "metadata": {
    "id": "u_5C4ygULVOL"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display the shape of the data\n",
    "\n",
    "We displayed the shape of the dataset to determine the number of rows and columns it contains. This information is essential for understanding the size and structure of the data, which can impact the analysis and modeling process."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_employee_data.describe()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display the columns of the data\n",
    "\n",
    "We displayed the column names of the dataset to identify the variables or features available for analysis. Understanding the columns helps in selecting relevant data for specific tasks, such as predictive modeling, clustering, or classification."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_employee_data.isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Drop the unnecessary columns\n",
    "\n",
    "We dropped the unnecessary columns from the dataset to focus on the relevant features for analysis. Removing redundant or irrelevant columns helps in reducing the dimensionality of the data and improving the performance of machine learning models.\n",
    "\n",
    "The columns `EmployeeCount`, `Over18`, and `StandardHours` were dropped as they contained constant values for all employees and did not provide any useful information for analysis.\n",
    "\n",
    "The `EmployeeID` column was also removed as it served as an identifier and was not required for further analysis."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# EmployeeCount : All values are 1\n",
    "# Over18 : All values are 'Y'\n",
    "# StandardHours : All values are 8\n",
    "columns_to_drop = ['EmployeeCount', 'Over18', 'StandardHours', 'EmployeeID']\n",
    "\n",
    "# Drop additional columns if is_ethic is True\n",
    "if is_ethic:\n",
    "    columns_to_drop += ['Age', 'Education', 'MaritalStatus', 'Gender', 'EducationField']\n",
    "\n",
    "full_employee_data = full_employee_data.drop(columns_to_drop, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encode the categorical data\n",
    "\n",
    "In this step, **we encoded categorical variables** in the employee dataset to prepare the data for machine learning models, which typically require numerical inputs. We targeted specific categorical columns: `Department`, `BusinessTravel`, `JobRole`, `MaritalStatus`, `Gender`, and `EducationField`.\n",
    "For each of these columns, we used **one-hot encoding**, a technique that converts each unique category into separate binary columns. For example, if the `Department` column contains values like \"Sales,\" \"HR,\" and \"R&D,\" one-hot encoding will create three new columns: `Department_Sales`, `Department_HR`, and `Department_R&D`. Each row will have a value of `1` in the relevant column and `0` in the others. This ensures that the categorical data is represented numerically without introducing any unintended ordinal relationships.\n",
    "\n",
    "To maintain flexibility, we first checked if each column in the list still existed in the dataset before applying the encoding. This avoids errors if columns were previously removed or altered. After encoding, the original categorical columns were dropped from the dataset to avoid redundancy. In addition to encoding, we transformed the **`Attrition`** column, which indicates whether an employee has left the company, into binary values.\n",
    "Specifically, we mapped `'Yes'` to `1` and `'No'` to `0`, making it suitable for classification models. This binary encoding enables the model to treat attrition as a target variable for predictive analysis. By the end of this process, all relevant categorical features and the target variable were represented numerically, ensuring the dataset was ready for further data processing or model training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Encoding\n",
    "cat_data = ['Department', 'BusinessTravel', 'JobRole', 'MaritalStatus', 'Gender', 'EducationField']\n",
    "\n",
    "# Only encode columns that are still present in the dataframe\n",
    "for i in cat_data:\n",
    "    if i in full_employee_data.columns:\n",
    "        vals = pd.get_dummies(full_employee_data[i], sparse=True)\n",
    "        full_employee_data = pd.concat([full_employee_data, vals], axis=1)\n",
    "        full_employee_data = full_employee_data.drop(i, axis=1)\n",
    "\n",
    "# Map Attrition column to binary values\n",
    "full_employee_data['Attrition'] = full_employee_data['Attrition'].map({'Yes': 1, 'No': 0})"
   ],
   "metadata": {
    "id": "VjBcyvMULXiP"
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Impute the missing values\n",
    "\n",
    "In this step, we **imputed missing values** in the dataset to ensure that the data was complete and ready for analysis. Missing values can occur due to various reasons, such as data entry errors, system failures, or incomplete records. Imputation is the process of filling in missing values with estimated or calculated values based on the available data.\n",
    "We used the **SimpleImputer** class from scikit-learn to impute missing values in the dataset. We chose the **median strategy** to replace missing values with the median of each column. The median is a robust measure of central tendency that is less sensitive to outliers than the mean. By imputing missing values with the median, we aimed to maintain the integrity of the data and avoid introducing bias or distortion.\n",
    "After imputing missing values, we checked the dataset for any remaining null values to confirm that the imputation process was successful. This ensured that the data was clean, complete, and ready for further analysis or modeling."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#Imputation\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "for cat_name in full_employee_data.columns:\n",
    "    full_employee_data[cat_name] = imputer.fit_transform(full_employee_data[[cat_name]])\n",
    "\n",
    "print(full_employee_data.isnull().sum())"
   ],
   "metadata": {
    "id": "JKtP8PjoLaVY"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scale the data\n",
    "\n",
    "In this step, we **scaled the data** to ensure that all features were on a similar scale, which is essential for many machine learning algorithms. Scaling the data helps in improving the performance and convergence of models by ensuring that no single feature dominates the others.\n",
    "We used the **MinMaxScaler** class from scikit-learn to scale the data to a specific range. Min-max scaling transforms the data to a specified range (default is 0 to 1) by subtracting the minimum value and dividing by the range. This normalization technique preserves the relationships between the data points while ensuring that all features are within the same scale.\n",
    "After scaling the data, we checked the summary statistics of the dataset to confirm that all features were scaled appropriately. This step ensured that the data was ready for further analysis, modeling, or visualization."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "for cat_name in full_employee_data.columns:\n",
    "    full_employee_data[cat_name] = scaler.fit_transform(full_employee_data[[cat_name]])\n",
    "full_employee_data"
   ],
   "metadata": {
    "id": "qNPFmGZ3Lbfw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the data\n",
    "print(full_employee_data.head())\n",
    "\n",
    "# Display the shape of the data\n",
    "print(full_employee_data.shape)\n",
    "\n",
    "# Display the columns of the data\n",
    "print(full_employee_data.columns)\n",
    "\n",
    "# Display the summary statistics of the data\n",
    "print(full_employee_data.describe(include='all'))\n",
    "\n",
    "# Display the missing values in the data\n",
    "print(full_employee_data.isnull().sum())\n",
    "\n",
    "# Display the unique values in the data\n",
    "full_employee_data.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Display the correlation matrix of the data\n",
    "corr_matrix = full_employee_data.corr()\n",
    "\n",
    "# Print the correlation of each data which is greater than 0.5 absolute\n",
    "high_corr = corr_matrix[(np.abs(corr_matrix) > 0.5) & (corr_matrix != 1.0)]\n",
    "print(high_corr.dropna(how='all', axis=0).dropna(how='all', axis=1))\n",
    "\n",
    "# Create a heatmap of the correlation matrix\n",
    "plt.figure(figsize=(20, 15), edgecolor='white')\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='YlGnBu', cbar=False)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "full_employee_data.to_csv('full_employee_data_cleaned.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  }
 ]
}
