{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Artificial Intelligence Project\n",
    "\n",
    "## Group 1:\n",
    "### - Rajoelisoa Enorian\n",
    "### - Hirli Baptiste\n",
    "### - Bhattacharjee Ankit\n",
    "### - Caumartin Evan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "### 1. Load the data\n",
    "### 2. Merge the data\n",
    "### 3. Calculate the average working time\n",
    "### 4. Remove the outliers\n",
    "### 5. Drop the unnecessary columns\n",
    "### 6. Encode the categorical data\n",
    "### 7. Impute the missing values\n",
    "### 8. Scale the data\n",
    "### 9. Save the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lVFYaugeLQiE"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dg2ybgF8LpMd"
   },
   "outputs": [],
   "source": [
    "full_employee_data: pd.DataFrame = pd.DataFrame()\n",
    "\n",
    "is_ethic = True\n",
    "remove_outliers = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load the data\n",
    "\n",
    "### Load the data from the datasets folder\n",
    "\n",
    "We read multiple CSV files into pandas DataFrames. The general_data, employee_survey_data, and manager_survey_data DataFrames contain general, employee survey, and manager survey data, respectively. The in_time and out_time DataFrames contain timestamp data for employee check-in and check-out times, which are converted to datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "general_data = pd.read_csv('datasets/general_data.csv')\n",
    "employee_survey_data = pd.read_csv('datasets/employee_survey_data.csv')\n",
    "manager_survey_data = pd.read_csv('datasets/manager_survey_data.csv')\n",
    "in_time: pd.DataFrame = pd.read_csv('datasets/in_time.csv', parse_dates=True)\n",
    "out_time = pd.read_csv('datasets/out_time.csv', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Merge the data\n",
    "\n",
    "We are merging general data, employee survey data, and manager survey data into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_employee_data = general_data.merge(employee_survey_data, on='EmployeeID')\n",
    "full_employee_data = full_employee_data.merge(manager_survey_data, on='EmployeeID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Calculate the average working time\n",
    "\n",
    "We analyzed employee working patterns by calculating key metrics such as average arrival time, departure time, and working hours. Starting with arrival (`in_time`) and departure (`out_time`) datasets, we determined the total time spent at work each day by subtracting arrival times from departure times and converting the results into hours.\n",
    "(Here, we assumed that the arrival and departure times were on the same day to simplify the analysis. If the data spanned multiple days, additional processing would be required to account for overnight shifts.)\n",
    "We then calculated the **AverageArrivalTime** (mean of arrival times), **AverageDepartureTime** (latest departure time), and **AverageWorkingTime** (average daily working hours) for each employee. These insights were combined into a new dataset and merged with the general employee data to provide a comprehensive view, including details like name and department alongside their working time metrics.\n",
    "For instance, an employee arriving at 9:15 AM and leaving at 6:30 PM consistently would have an average working time of 9 hours. This analysis helps in understanding employee work habits, identifying trends, and improving operational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "in_time = in_time.fillna('1970-01-01 00:00:00')\n",
    "out_time = out_time.fillna('1970-01-01 00:00:00')\n",
    "\n",
    "# Convert all times to datetime\n",
    "in_time.iloc[:, 1:] = in_time.iloc[:, 1:].apply(pd.to_datetime).apply(lambda x: x.dt.hour * 3600 + x.dt.minute * 60 + x.dt.second)\n",
    "out_time.iloc[:, 1:] = out_time.iloc[:, 1:].apply(pd.to_datetime).apply(lambda x: x.dt.hour * 3600 + x.dt.minute * 60 + x.dt.second)\n",
    "\n",
    "# Replace 0 with row medians using mask or where to avoid the FutureWarning\n",
    "in_time.iloc[:, 1:] = in_time.iloc[:, 1:].apply(lambda row: row.mask(row == 0, row.median()), axis=1)\n",
    "out_time.iloc[:, 1:] = out_time.iloc[:, 1:].apply(lambda row: row.mask(row == 0, row.median()), axis=1)\n",
    "\n",
    "# Calculate working time for each day in hours\n",
    "working_time = (out_time.iloc[:, 1:] - in_time.iloc[:, 1:])\n",
    "\n",
    "# Average arrival time\n",
    "average_arrival_time = in_time.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "# Average departure time\n",
    "average_departure_time = out_time.iloc[:, 1:].max(axis=1)\n",
    "\n",
    "# Calculate average working time\n",
    "average_working_time = working_time.mean(axis=1) / 3600\n",
    "\n",
    "# Convert average times directly to total seconds\n",
    "result = pd.DataFrame({\n",
    "    'EmployeeID': in_time.iloc[:, 0],  # Use the first column directly\n",
    "    'AverageArrivalTime': average_arrival_time.round(0).astype(int),\n",
    "    'AverageDepartureTime': average_departure_time.round(0).astype(int),\n",
    "    'AverageWorkingTime': average_working_time.round(2)\n",
    "})\n",
    "\n",
    "result.to_csv('working_time_data.csv', index=False)\n",
    "\n",
    "# Merge with other employee data\n",
    "full_employee_data = full_employee_data.merge(result, on='EmployeeID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Remove the outliers\n",
    "\n",
    "We removed the outliers from the dataset using the Interquartile Range (IQR) method. Outliers are data points that significantly differ from other observations in the dataset. They can skew the results of statistical analyses and machine learning models.\n",
    "The IQR method identifies outliers by calculating the range between the first and third quartiles of the data and flagging values that fall below the lower bound (Q1 - 1.5 * IQR) or above the upper bound (Q3 + 1.5 * IQR). Outliers are then replaced with the lower or upper bound values to ensure they do not impact the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_outliers_iqr(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = (series < lower_bound) | (series > upper_bound)\n",
    "    return outliers\n",
    "\n",
    "def cap_outliers_in_dataframe(df, excluded_columns=None):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if not remove_outliers:\n",
    "        return df\n",
    "\n",
    "    for column in df.columns:\n",
    "        if excluded_columns and column in excluded_columns:\n",
    "            continue\n",
    "        \n",
    "        if np.issubdtype(df[column].dtype, np.number):  # Check if the column contains numeric data\n",
    "            outliers = find_outliers_iqr(df[column])\n",
    "\n",
    "            # Cap the outliers by replacing them with lower/upper bounds\n",
    "            Q1 = df[column].quantile(0.25)\n",
    "            Q3 = df[column].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "            df[column] = np.where(outliers, np.clip(df[column], lower_bound, upper_bound), df[column])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_employee_data = cap_outliers_in_dataframe(full_employee_data, excluded_columns=['PerformanceRating'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Display the first 5 rows of the data\n",
    "\n",
    "We displayed the first five rows of the dataset to understand its structure and contents. This step provides an overview of the data, including the column names, data types, and values. It helps in identifying any potential issues, such as missing values, outliers, or incorrect data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_5C4ygULVOL"
   },
   "outputs": [],
   "source": [
    "full_employee_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Display the shape of the data\n",
    "\n",
    "We displayed the shape of the dataset to determine the number of rows and columns it contains. This information is essential for understanding the size and structure of the data, which can impact the analysis and modeling process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_employee_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Display the columns of the data\n",
    "\n",
    "We displayed the column names of the dataset to identify the variables or features available for analysis. Understanding the columns helps in selecting relevant data for specific tasks, such as predictive modeling, clustering, or classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_employee_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Drop the unnecessary columns\n",
    "\n",
    "We dropped the unnecessary columns from the dataset to focus on the relevant features for analysis. Removing redundant or irrelevant columns helps in reducing the dimensionality of the data and improving the performance of machine learning models.\n",
    "\n",
    "The columns `EmployeeCount`, `Over18`, and `StandardHours` were dropped as they contained constant values for all employees and did not provide any useful information for analysis.\n",
    "\n",
    "The `EmployeeID` column was also removed as it served as an identifier and was not required for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# EmployeeCount : All values are 1\n",
    "# Over18 : All values are 'Y'\n",
    "# StandardHours : All values are 8\n",
    "columns_to_drop = ['EmployeeCount', 'Over18', 'StandardHours', 'EmployeeID']\n",
    "\n",
    "# Drop additional columns if is_ethic is True\n",
    "if is_ethic:\n",
    "    columns_to_drop += ['Age', 'Education', 'MaritalStatus', 'Gender', 'EducationField']\n",
    "\n",
    "full_employee_data = full_employee_data.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Encode the categorical data\n",
    "\n",
    "In this step, **we encoded categorical variables** in the employee dataset to prepare the data for machine learning models, which typically require numerical inputs. We targeted specific categorical columns: `Department`, `BusinessTravel`, `JobRole`, `MaritalStatus`, `Gender`, and `EducationField`.\n",
    "For each of these columns, we used **one-hot encoding**, a technique that converts each unique category into separate binary columns. For example, if the `Department` column contains values like \"Sales,\" \"HR,\" and \"R&D,\" one-hot encoding will create three new columns: `Department_Sales`, `Department_HR`, and `Department_R&D`. Each row will have a value of `1` in the relevant column and `0` in the others. This ensures that the categorical data is represented numerically without introducing any unintended ordinal relationships.\n",
    "\n",
    "To maintain flexibility, we first checked if each column in the list still existed in the dataset before applying the encoding. This avoids errors if columns were previously removed or altered. After encoding, the original categorical columns were dropped from the dataset to avoid redundancy. In addition to encoding, we transformed the **`Attrition`** column, which indicates whether an employee has left the company, into binary values.\n",
    "Specifically, we mapped `'Yes'` to `1` and `'No'` to `0`, making it suitable for classification models. This binary encoding enables the model to treat attrition as a target variable for predictive analysis. By the end of this process, all relevant categorical features and the target variable were represented numerically, ensuring the dataset was ready for further data processing or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VjBcyvMULXiP"
   },
   "outputs": [],
   "source": [
    "#Encoding\n",
    "cat_data = ['Department', 'BusinessTravel', 'JobRole', 'MaritalStatus', 'Gender', 'EducationField']\n",
    "\n",
    "# Only encode columns that are still present in the dataframe\n",
    "for i in cat_data:\n",
    "    if i in full_employee_data.columns:\n",
    "        vals = pd.get_dummies(full_employee_data[i], sparse=True)\n",
    "        full_employee_data = pd.concat([full_employee_data, vals], axis=1)\n",
    "        full_employee_data = full_employee_data.drop(i, axis=1)\n",
    "\n",
    "# Map Attrition column to binary values\n",
    "full_employee_data['Attrition'] = full_employee_data['Attrition'].map({'Yes': 1, 'No': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Impute the missing values\n",
    "\n",
    "In this step, we **imputed missing values** in the dataset to ensure that the data was complete and ready for analysis. Missing values can occur due to various reasons, such as data entry errors, system failures, or incomplete records. Imputation is the process of filling in missing values with estimated or calculated values based on the available data.\n",
    "We used the **SimpleImputer** class from scikit-learn to impute missing values in the dataset. We chose the **median strategy** to replace missing values with the median of each column. The median is a robust measure of central tendency that is less sensitive to outliers than the mean. By imputing missing values with the median, we aimed to maintain the integrity of the data and avoid introducing bias or distortion.\n",
    "After imputing missing values, we checked the dataset for any remaining null values to confirm that the imputation process was successful. This ensured that the data was clean, complete, and ready for further analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKtP8PjoLaVY"
   },
   "outputs": [],
   "source": [
    "#Imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "for cat_name in full_employee_data.columns:\n",
    "    full_employee_data[cat_name] = imputer.fit_transform(full_employee_data[[cat_name]])\n",
    "\n",
    "print(full_employee_data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Scale the data\n",
    "\n",
    "In this step, we **scaled the data** to ensure that all features were on a similar scale, which is essential for many machine learning algorithms. Scaling the data helps in improving the performance and convergence of models by ensuring that no single feature dominates the others.\n",
    "We used the **MinMaxScaler** class from scikit-learn to scale the data to a specific range. Min-max scaling transforms the data to a specified range (default is 0 to 1) by subtracting the minimum value and dividing by the range. This normalization technique preserves the relationships between the data points while ensuring that all features are within the same scale.\n",
    "After scaling the data, we checked the summary statistics of the dataset to confirm that all features were scaled appropriately. This step ensured that the data was ready for further analysis, modeling, or visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qNPFmGZ3Lbfw"
   },
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "no_scale = [\"PerformanceRating\", \"Attrition\"]\n",
    "\n",
    "for cat_name in full_employee_data.columns:\n",
    "    if cat_name not in no_scale:\n",
    "        full_employee_data[cat_name] = scaler.fit_transform(full_employee_data[[cat_name]])\n",
    "    \n",
    "print(full_employee_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Display the first 5 rows of the data\n",
    "print(full_employee_data.head())\n",
    "\n",
    "# Display the shape of the data\n",
    "print(full_employee_data.shape)\n",
    "\n",
    "# Display the columns of the data\n",
    "print(full_employee_data.columns)\n",
    "\n",
    "# Display the summary statistics of the data\n",
    "print(full_employee_data.describe(include='all'))\n",
    "\n",
    "# Display the missing values in the data\n",
    "print(full_employee_data.isnull().sum())\n",
    "\n",
    "# Display the unique values in the data\n",
    "full_employee_data.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def display_correlation_matrix(data: pd.DataFrame):\n",
    "    # Display the correlation matrix of the data\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    # Create a heatmap of the correlation matrix\n",
    "    plt.figure(figsize=(20, 15), edgecolor='white')\n",
    "    sns.heatmap(corr_matrix, annot=True, fmt='.3f', cmap='YlGnBu', cbar=False)\n",
    "    plt.show()\n",
    "\n",
    "display_correlation_matrix(full_employee_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Remove the columns with high correlation\n",
    "\n",
    "We removed the columns with high correlation to avoid multicollinearity, which can lead to unstable model coefficients and inaccurate predictions. Multicollinearity occurs when two or more independent variables are highly correlated, making it difficult to determine the individual effect of each variable on the target variable.\n",
    "We identified the highly correlated columns using the correlation matrix and removed them from the dataset. This step helped in improving the model's performance and interpretability by eliminating redundant features and reducing the complexity of the data.\n",
    "We removed the columns `Travel_Rarely`, `AverageDepartureTime`, `YearsWithCurrManager`, `Sales` and `PerformanceRating` as they were highly correlated with other features in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove the columns with high correlation\n",
    "full_employee_data = full_employee_data.drop(\"Travel_Rarely\", axis=1)\n",
    "full_employee_data = full_employee_data.drop(\"AverageDepartureTime\", axis=1)\n",
    "full_employee_data = full_employee_data.drop(\"YearsWithCurrManager\", axis=1)\n",
    "full_employee_data = full_employee_data.drop(\"Sales\", axis=1)\n",
    "full_employee_data = full_employee_data.drop(\"PerformanceRating\", axis=1)\n",
    "\n",
    "display_correlation_matrix(full_employee_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "full_employee_data.to_csv('full_employee_data_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Testing Different ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### *Linear Kernel Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from models.statistics_helper import StatisticsHelper\n",
    "\n",
    "# Load the data\n",
    "full_employee_data = pd.read_csv('./full_employee_data_cleaned.csv')\n",
    "\n",
    "# Prepare the data\n",
    "target_column = 'Attrition'\n",
    "X = full_employee_data.drop(target_column, axis=1)\n",
    "y = full_employee_data[target_column]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "lr_model = LinearSVC(random_state=42, dual='auto')\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Display the statistics\n",
    "stats_helper = StatisticsHelper(X, y, lr_model, y_test, y_pred)\n",
    "lr_acc = stats_helper.ret_cross_val_score()\n",
    "lr_roc = stats_helper.ret_roc_auc_score()\n",
    "stats_helper.show_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### *Logistic Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)  # Increase max_iter if convergence warnings occur\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "\n",
    "# Display the statistics\n",
    "stat_helper = StatisticsHelper(X, y, log_reg_model, y_test, y_pred)\n",
    "log_acc = stats_helper.ret_cross_val_score()\n",
    "log_roc = stats_helper.ret_roc_auc_score()\n",
    "stat_helper.show_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Decison Tree*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = dt_model.predict(X_test)\n",
    "\n",
    "# Display the statistics\n",
    "stats_helper = StatisticsHelper(X_train, y_train, dt_model, y_test, y_pred)\n",
    "dt_acc = stats_helper.ret_cross_val_score()\n",
    "dt_roc = stats_helper.ret_roc_auc_score()\n",
    "stats_helper.show_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### *Random Forest*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=1000, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Display the statistics\n",
    "stats_helper = StatisticsHelper(X_train, y_train, rf_model, y_test, y_pred)\n",
    "rf_acc = stats_helper.ret_cross_val_score()\n",
    "rf_roc = stats_helper.ret_roc_auc_score()\n",
    "stats_helper.show_all()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### *K-Nearest-Neighbor*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Testing for the optimal n_neighbors value to use in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#Test for the best n_neighbors value\n",
    "for i in range(1,20):\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "\n",
    "    #Make predictions\n",
    "    y_pred = knn_model.predict(X_test)\n",
    "    stats_helper = StatisticsHelper(X, y, knn_model, y_test, y_pred)\n",
    "    print(f\"N = {i}\")\n",
    "    stats_helper.show_accuracy()\n",
    "    stats_helper.show_cross_val_score()\n",
    "    stats_helper.show_roc_auc_score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### It can be seen from the results above that N = 1,2,3 has the best results but is not reliable in a more varied dataset as it basically just picks the number next to it. N = 7 appears to reach a good result among the rest so that is that we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors=7)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = knn_model.predict(X_test)\n",
    "\n",
    "stats_helper = StatisticsHelper(X, y, knn_model, y_test, y_pred)\n",
    "knn_acc = stats_helper.ret_cross_val_score()\n",
    "knn_roc = stats_helper.ret_roc_auc_score()\n",
    "stats_helper.show_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### *Support Vector Machine (Non-Linear)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Performing Grid Search will help us find optimal parameters to be used for the Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.svm import SVC\n",
    "\n",
    "param_grid = {\n",
    "    'C': [1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
    "    'gamma': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "svm = SVC(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=svm, param_grid=param_grid, cv=5, verbose=1, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Display the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### The parameters give above were chosen after some trial and error. C values below 1 and above 10 were insignificant or redundent. Gamma values below 0.01 would lead to minimal learning of the negatives and above 0.1 would lead to clear overfitting. Hence among the given values, the best model was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_model = grid_search.best_estimator_\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "stats_helper = StatisticsHelper(X, y, svm_model, y_test, y_pred)\n",
    "svm_acc = stats_helper.ret_cross_val_score()\n",
    "svm_roc = stats_helper.ret_roc_auc_score()\n",
    "stats_helper.show_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Simple Neural Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Sequential\n",
    "from keras.api.layers import Dense\n",
    "\n",
    "nn_model = Sequential([\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(32, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are using standard hyperparameters for this model, using the loss function of binary cross entropy since it fits this situation. The model has 3 layers of neurons, with the final layer having a single output with the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The number of epochs here was chosen where the accuracy was sufficiently high. Increasing the number of epochs further makes the model perfectly accurate, however it would certainly be overfitted. Hence to avoid that, it has been kept at a sufficient level for this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = nn_model.fit(X_train, y_train, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formatting the float values to their respective integer class values using a threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nn_model.predict(X_test)\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] < 0.5:\n",
    "        y_pred[i] = 0\n",
    "    else:\n",
    "        y_pred[i] = 1\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_helper = StatisticsHelper(X, y, nn_model, y_test, y_pred)\n",
    "nn_acc = stats_helper.ret_accuracy()\n",
    "nn_roc = stats_helper.ret_roc_auc_score()\n",
    "stats_helper.show_accuracy()\n",
    "stats_helper.show_roc_auc_score()\n",
    "stats_helper.show_classification_report()\n",
    "stats_helper.show_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing models\n",
    "\n",
    "models = ['Linear Kernal Classification', 'Logistic Regression', 'Decision Tree Classifier', 'Random Forest Classifier', 'K-Nearest_Neighbor Classifier', 'Support Vector Machine (Non-Linear)', 'Neural Network']\n",
    "res_vals = [[lr_acc, log_acc, dt_acc, rf_acc, knn_acc, svm_acc, nn_acc],\n",
    "            [lr_roc, log_roc, dt_roc, rf_roc, knn_roc, svm_roc, nn_roc]]\n",
    "\n",
    "res_df = pd.DataFrame(res_vals, columns=models, index=['Accuracy', 'ROC_AUC'])\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(res_df, annot=True, cmap='Blues', fmt='.3g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From these results, it is clear that Random Forest Classifier and Neural Network give the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Conclusion\n",
    "\n",
    "The analysis of employee attrition at HumanForYou revealed critical factors influencing turnover, such as job satisfaction, work-life balance, frequent business travel, and tenure. Employees with low satisfaction scores, higher commute distances, or limited career growth opportunities showed a significantly higher likelihood of leaving the company. Additionally, certain job roles and hierarchical levels exhibited disproportionately high attrition rates, indicating specific areas requiring targeted interventions.\n",
    "\n",
    "To address these challenges, the company should focus on improving work-life balance and job satisfaction through initiatives such as flexible schedules, enhanced benefits, and professional development programs. Leveraging predictive models can enable the identification of at-risk employees, allowing proactive retention strategies. By adopting these measures, HumanForYou can reduce attrition, enhance project delivery timelines, and optimize resource allocation for recruitment and training.\n",
    "\n",
    "By leveraging predictive analytics, the company can identify employees most at risk of leaving and implement targeted retention strategies. These efforts, coupled with fostering a culture of continuous improvement, will not only reduce turnover but also strengthen employee morale and productivity. With these measures, HumanForYou can ensure the stability of its workforce, enhance project delivery, and build a strong reputation as a desirable workplace in the competitive pharmaceutical industry."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
